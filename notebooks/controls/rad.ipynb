{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47dafb6",
   "metadata": {},
   "source": [
    "<img src=\"../images/rad.png\" alt=\"RAD method\" width=\"500\"/>\n",
    "\n",
    "## Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model\n",
    "\n",
    "**Authors**: Haikang Deng, Colin Raffel\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2310.09520\n",
    "* Repo: https://github.com/r-three/RAD?tab=readme-ov-file\n",
    "\n",
    "RAD is an output steering method, enabling the users to perform controlled text generation with a unidirectional reward model. \n",
    "\n",
    "In this demo, we show how RAD can be used to reduce the toxicity of sentences generated by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63ed70",
   "metadata": {},
   "source": [
    "### Method parameters\n",
    "\n",
    "| parameter     | type            | description                                                                                   |\n",
    "| ------------- | --------------- | --------------------------------------------------------------------------------------------- |\n",
    "| `beta`        | `float`         | Steering intensity. Must be non-negative.                                                     |\n",
    "| `reward_path` | `Optional[str]` | Path to the trained reward model. See the [RAD repo](https://github.com/r-three/RAD) for details. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790838fe",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "The following authentication steps may be necessary to access any gated models (even after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub using your token stored in the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c04b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae412c",
   "metadata": {},
   "source": [
    "## Example: Steering for reduced toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c192ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.output_control.rad.control import RAD\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "MODEL_NAME = \"gpt2-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b6979",
   "metadata": {},
   "source": [
    "We initialize the RAD method with specified parameters. \n",
    "\n",
    "Below, `beta` represents the steering strength with `0` being the original decoding. \n",
    "\n",
    "RAD requires a trained reward model. In this demo, we will use the toxicity reward model provided by the authors. Please pass the path to the reward model via `reward_path`. If you don't pass the path, the reward model will be automatically downloaded to './rad_saved_models/saved_models/gpt2_toxicity'. To train your own reward model, please see https://github.com/r-three/RAD?tab=readme-ov-file for details. \n",
    "\n",
    "**Note**: You can adjust `beta` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3edc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = RAD(\n",
    "    beta=10,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae96a7e",
   "metadata": {},
   "source": [
    "If the reward model is already downloaded, please pass the path via `reward_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea4d08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rad = RAD(\n",
    "#     beta=10,\n",
    "#     reward_path='./rad_saved_models/saved_models/gpt2_toxicity',\n",
    "# ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28dc65",
   "metadata": {},
   "source": [
    "We create the `SteeringPipeline` object as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86f0d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://storage.googleapis.com/rad_release/saved_models.zip\n",
      "To: /dccstor/principled_ai/users/erikmiehling/AISteer360/tmp/rad_saved_models.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model not found in: None. Downloading from https://github.com/r-three/RAD......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 925M/925M [00:04<00:00, 200MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model downloaded. Please set reward_path='./tmp/rad_saved_models/saved_models/gpt2_toxicity' in the future.\n",
      "Reward model is loaded.\n"
     ]
    }
   ],
   "source": [
    "rad_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[rad],\n",
    "    device=\"cuda\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")\n",
    "rad_pipeline.steer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cf2cc",
   "metadata": {},
   "source": [
    "#### Controlled text generation via RAD steering\n",
    "\n",
    "Now, let's create a prompt for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f035bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = rad_pipeline.tokenizer\n",
    "\n",
    "PROMPT = \"I hate you. You are being like a drama queen who seeks attention all the time. I wish you\"\n",
    "\n",
    "enc = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "input_ids = enc.input_ids\n",
    "attention_mask = enc.attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27bbe7",
   "metadata": {},
   "source": [
    "We define the generation hyperparameters as follows, and generate the sentence continuation with RAD steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "932882b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered response (RAD, beta=10):  would just go away.\"\n",
      "\n",
      "\"You're right,\" he said, \"but it's not\n"
     ]
    }
   ],
   "source": [
    "gen_params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "steered_output_ids = rad_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "steered_response = tokenizer.decode(steered_output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Steered response (RAD, beta=10): {steered_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828201b3",
   "metadata": {},
   "source": [
    "#### Comparison (Optional)\n",
    "\n",
    "Users can also readily compare the continuation generation without RAD steering by setting `beta = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3095cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model found in: ./tmp/rad_saved_models/saved_models/gpt2_toxicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model is loaded.\n",
      "Steered response (RAD, beta=0):  would just shut up and go away.\"\n",
      "\n",
      "\"You're right,\" said the woman, \"\n"
     ]
    }
   ],
   "source": [
    "rad = RAD(\n",
    "    beta=0,\n",
    "    reward_path='./tmp/rad_saved_models/saved_models/gpt2_toxicity',\n",
    ")\n",
    "\n",
    "rad_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[rad],\n",
    "    device=\"cuda\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")\n",
    "\n",
    "rad_pipeline.steer()\n",
    "\n",
    "original_output_ids = rad_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "original_response = tokenizer.decode(original_output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Steered response (RAD, beta=0): {original_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
