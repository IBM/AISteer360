{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b03499",
   "metadata": {},
   "source": [
    "# Running TRL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85443d1e",
   "metadata": {},
   "source": [
    "The toolkit implements some of the [TRL](https://github.com/huggingface/trl) methods via a `StructuralControl` wrapper. This guide shows how to run several methods:\n",
    "\n",
    "- SFT (supervised fine-tuning)\n",
    "- DPO (direct preference optimization)\n",
    "- APO (anchored preference optimization).\n",
    "- SPPO (self-play preference optimization)\n",
    "\n",
    "Note that while [SPPO](https://github.com/uclaml/SPPO) is not a part of TRL, it follows many of the similar abstractions so we include it as part of our TRL wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e084a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fdfa5",
   "metadata": {},
   "source": [
    "If running this from a Google Colab notebook, please uncomment the following cell to install the toolkit. The following block is not necessary if running this notebook from a virtual environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c314665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/AISteer360.git\n",
    "# %cd AISteer360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd683f7c",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d917bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install ipywidgets\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4089bf8",
   "metadata": {},
   "source": [
    "Next, we import the `SteeringPipeline` class (used throughout) and specify the base model, in this case a small Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea0c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import PeftType\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663317c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd798d2",
   "metadata": {},
   "source": [
    "The controls throughout this notebook are trained using a common dataset, `ultrafeedback_binarized`, since it contains preference data for each prompt (which is necessary for DPO-based controls). We load each of the splits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30c35fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61135,\n",
       " dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "raw_test  = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"test_prefs\")\n",
    "len(raw_train), raw_train[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77232ea0",
   "metadata": {},
   "source": [
    "Different trainers expect different data formats (i.e., tensor layouts) and thus we define two helper functions, one for SFT and one for DPO, to process the data in a way that is amenable to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed94077a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sft_preprocess(example, tokenizer, max_length=1024):\n",
    "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['chosen']}\"\n",
    "    encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    labels = [\n",
    "        token_id if mask == 1 else -100. # label pads as -100 so they don't contribute to loss\n",
    "        for token_id, mask in zip(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "    ]\n",
    "    encoding[\"labels\"] = labels\n",
    "    return encoding\n",
    "\n",
    "def dpo_filter(example, max_prompt_chars=4000):\n",
    "    prompt = example[\"prompt\"]\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        prompt = prompt[:max_prompt_chars]\n",
    "    return {\"prompt\": prompt, \"chosen\": example[\"chosen\"], \"rejected\": example[\"rejected\"]}\n",
    "\n",
    "\n",
    "subset_size = 500\n",
    "\n",
    "sft_train = raw_train.select(range(subset_size)).map(\n",
    "    lambda example: sft_preprocess(example, tokenizer, max_length=1024),\n",
    "    remove_columns=raw_train.column_names\n",
    ")\n",
    "\n",
    "dpo_train = raw_train.select(range(subset_size)).map(dpo_filter, remove_columns=[])\n",
    "dpo_train[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e1702",
   "metadata": {},
   "source": [
    "## SFT control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7c8de",
   "metadata": {},
   "source": [
    "We now show how to fine-tune with SFT using LoRA. We also merge the trained adapter back into the model (using the argument `merge_lora_after_train`). Note the argument `use_peft=True` to indicate that we are not running a full fine-tune (the example near the end of this notebook will illustrate a full fine-tuning run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c89a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.sfttrainer.control import SFT\n",
    "\n",
    "\n",
    "sft = SFT(\n",
    "    # control loads model/tokenizer\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "\n",
    "    # data\n",
    "    train_dataset=sft_train,\n",
    "    eval_dataset=None, \n",
    "    # data_collator=None  # optional; if omitted and you provided labels, you're fine\n",
    "\n",
    "    # TRL / Trainer config (forwarded into SFTConfig)\n",
    "    output_dir=\"./tmp/sft_lora\",\n",
    "    max_seq_length=1024,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "\n",
    "    # PEFT (LoRA)\n",
    "    use_peft=True,\n",
    "    peft_type=PeftType.LORA,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    adapter_name=\"sft\",\n",
    "\n",
    "    # optionally merge LoRA into base weights after training\n",
    "    merge_lora_after_train=True,\n",
    "    merged_output_dir=\"./tmp/sft_lora_merged\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e64fd",
   "metadata": {},
   "source": [
    "We create a steering pipeline using the above control, with `lazy_init=True` since the structural control (`sft`) returns a model. The pipeline is then steered which invokes the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb6f6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_pipeline = SteeringPipeline(\n",
    "    lazy_init=True,\n",
    "    controls=[sft],\n",
    ")\n",
    "sft_pipeline.steer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a256987",
   "metadata": {},
   "source": [
    "The above SFT-trained pipeline is now ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b19d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The sky looks blue because of the scattering of light by tiny dust particles in the atmosphere. These particles are small and light, so they scatter the light that hits them, causing it to bend around them and spread out into a colorless, milky cloud-like appearance known as the \"blue\" part of the sky.']\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Question: What makes the sky look blue?\\n\\nAnswer:\"\n",
    "encoded = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "text = sft_pipeline.generate_text(\n",
    "    input_ids=encoded[\"input_ids\"],\n",
    "    attention_mask=encoded[\"attention_mask\"],\n",
    "    max_new_tokens=64\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d17982",
   "metadata": {},
   "source": [
    "## DPO control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecba008",
   "metadata": {},
   "source": [
    "DPO is instantiated in a similar fashion with the primary differences being that the training data is now triples (`prompt`, `chosen`, `rejected`), the trainer must keep a reference policy alongside the trainable policy, and the loss is a pair-wise KL-reg. contrastive objective rather than the token-level cross entropy loss in SFT. \n",
    "\n",
    "Note: By default, the trainer clones the base weights and freezes them. When LoRA is enabled, the wrapper automatically passes `ref_model=None`, letting TRL re-create a frozen reference that shares the same LoRA adapters. If you are full fine-tuning you can still supply your own `ref_model` via `pipeline.steer(ref_model=my_frozen_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a33f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO\n",
    "\n",
    "\n",
    "dpo = DPO(\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "\n",
    "    train_dataset=dpo_train,\n",
    "\n",
    "    # DPO / TRL config (forwarded into DPOConfig)\n",
    "    output_dir=\"./tmp/dpo_lora\",\n",
    "    per_device_train_batch_size=2,  # often smaller than SFT\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-6,\n",
    "    beta=0.1,\n",
    "    loss_type=\"sigmoid\",  # baseline DPO loss\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "    precompute_ref_log_probs=True,  # forwarded if supported by your TRL version\n",
    "    disable_dropout=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=123,\n",
    "\n",
    "    # LoRA\n",
    "    use_peft=True,\n",
    "    peft_type=PeftType.LORA,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    adapter_name=\"dpo\",\n",
    "\n",
    "    merge_lora_after_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e56422",
   "metadata": {},
   "source": [
    "As before, we create the pipeline using the control, steer the pipeline, and run inference on the steered pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc5a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Train dataset reference log probs:  25%|██▍       | 62/250 [00:29<01:29,  2.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dataset reference log probs: 100%|██████████| 250/250 [01:54<00:00,  2.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.037700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_pipeline = SteeringPipeline(\n",
    "    lazy_init=True, \n",
    "    controls=[dpo]\n",
    ")\n",
    "dpo_pipeline.steer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e91b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Yes, it is always helpful to be blunt with feedback. Blunt feedback can help you identify areas of improvement and provide a clear path for change. It also helps to build trust between the person being evaluated and the person giving the feedback.\\n\\nFor example, if someone gives you feedback that says \"You need to improve your writing skills,\" you could respond by saying \"I agree, but I think we should focus on improving our research methods instead.\" This response provides constructive criticism without sounding accusatory or dismissive.\\n\\nBlunt feedback can also help to motivate people to take action towards their goals. If someone gives you feedback that says \"You need to work harder on this project,\" you could say \"Thank you for your input, but I think we can']\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Question: Is it ever helpful to be blunt with feedback?\\n\\nAnswer:\"\n",
    "encoded = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "print(dpo_pipeline.generate_text(\n",
    "    input_ids=encoded[\"input_ids\"],\n",
    "    attention_mask=encoded[\"attention_mask\"],\n",
    "    max_new_tokens=150,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93f524",
   "metadata": {},
   "source": [
    "## APO control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3048e",
   "metadata": {},
   "source": [
    "APO lives in the same trainer family as DPO and uses the same `DPOTrainer` class (it is activated by simply choosing a different `loss_type`). In contrast to DPO that pushes the policy away from the reference (by a relative KL-scaled margin), APO pushes the policy toward a fixed \"anchor\" score. Generally, APO keeps the policy closer to the reference for the same beta, reducing the risk of over-optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2f61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.apotrainer.control import APO\n",
    "\n",
    "\n",
    "apo = APO(\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "\n",
    "    train_dataset=dpo_train,\n",
    "\n",
    "    output_dir=\"./tmp/apo_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-6,\n",
    "    beta=0.1,\n",
    "    loss_type=\"apo_zero\",     # APO-specific loss\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=99,\n",
    "\n",
    "    use_peft=True,\n",
    "    peft_type=PeftType.LORA,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    adapter_name=\"apo\",\n",
    "\n",
    "    merge_lora_after_train=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26db93",
   "metadata": {},
   "source": [
    "Steering and inference proceeds as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc765081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Train dataset reference log probs: 100%|██████████| 250/250 [01:54<00:00,  2.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.004500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apo_pipeline = SteeringPipeline(\n",
    "    lazy_init=True, \n",
    "    controls=[apo]\n",
    ")\n",
    "apo_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c62491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Kindness is a powerful tool that can be used strategically in various situations. It allows us to connect with others, build trust and relationships, and promote positive change. By being kind, we can create a positive impact on the world and help others in need. Additionally, kindness can be used as a way to set an']\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"Question: Explain why kindness can be strategic.\\n\\nAnswer:\"\n",
    "encoded = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "print(apo_pipeline.generate_text(\n",
    "    input_ids=encoded[\"input_ids\"],\n",
    "    attention_mask=encoded[\"attention_mask\"],\n",
    "    max_new_tokens=64,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69509bb",
   "metadata": {},
   "source": [
    "## SPPO control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff458b46",
   "metadata": {},
   "source": [
    "SPPO, or self-play preference optimization, can be thought of as extending the offline DPO setting into an on-policy, self-improving loop. The data starts with only a prompt corpus (no human-written answers required). During trainin the policy generates two candidate answers itself. Next, a preference model (or a heuristic judge) ranks the two self-generated candidates. The chosen-rejected is then fed through the DPO-style loss.\n",
    "\n",
    "Because the answers were sampled from the current policy, the optimization is on-policy with the model producing new pairs every few steps so it continuously trains on its own mistakes. A reference model is still necessary to stabilize learning.\n",
    "\n",
    "SPPO is implemented via `SPPOTrainer` and uses the same `DPOTrainerMixin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16889b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmp2jpmt58a\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (80.9.0)\n",
      "Processing /tmp/tmp2jpmt58a/pip-24.0-py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-24.0\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.2\n",
      "Collecting llm-blender\n",
      "  Using cached llm_blender-0.0.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (from llm-blender) (4.57.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from llm-blender) (2.9.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llm-blender) (2.3.4)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (from llm-blender) (1.3.0)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.11/site-packages (from llm-blender) (0.4.5)\n",
      "Collecting dataclasses-json (from llm-blender)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from llm-blender) (0.2.1)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from llm-blender) (6.32.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from accelerate->llm-blender) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate->llm-blender) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from accelerate->llm-blender) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.11/site-packages (from accelerate->llm-blender) (0.35.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (2025.3.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate->llm-blender) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.11/site-packages (from torch->llm-blender) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch->llm-blender) (1.3.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llm-blender)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->llm-blender)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->llm-blender)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->llm-blender) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->llm-blender) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->llm-blender) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->llm-blender) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->llm-blender) (2025.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers->llm-blender) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers->llm-blender) (0.22.1)\n",
      "Using cached llm_blender-0.0.2-py3-none-any.whl (92 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, llm-blender\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [llm-blender]\u001b[0m [llm-blender]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dataclasses-json-0.6.7 llm-blender-0.0.2 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m ensurepip --upgrade\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install llm-blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4075c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.sppotrainer.control import SPPO\n",
    "\n",
    "\n",
    "subset = raw_train.select(range(200)).map(lambda ex: {\"prompt\": ex[\"prompt\"]}, remove_columns=raw_train.column_names)\n",
    "\n",
    "sppo = SPPO(\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "    train_dataset=subset,\n",
    "\n",
    "    # SPPO params\n",
    "    start_iteration=1,\n",
    "    end_iteration=1,\n",
    "    max_input_length=1024,\n",
    "    num_prompts=2,  #5,\n",
    "    temp_dir=\"./tmp/sppo_temp\",\n",
    "    gen_max_new_tokens=32,  #100,\n",
    "    ranking_batch_size=8,\n",
    "    limit_num_examples=20,  #50,\n",
    "\n",
    "    # TRL/DPO-compatible params\n",
    "    output_dir=\"./tmp/sppo_final\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    beta=0.001,\n",
    "    loss_type=\"sppo\",\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=123,\n",
    "\n",
    "    # # LoRA (optional)\n",
    "    # use_peft=True,\n",
    "    # peft_type=PeftType.LORA,\n",
    "    # r=16,\n",
    "    # lora_alpha=16,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    # adapter_name=\"sppo\",\n",
    "    # merge_lora_after_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea3895",
   "metadata": {},
   "source": [
    "We can now construct a steering pipeline, steer it (runs one SPPO iteration, saves checkpoint and final model), and run inference on the steered pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c509a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /dccstor/principled_ai/users/erikmiehling/huggingface_cache/hub/llm-blender/PairRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "Generating train split: 20 examples [00:00, 4043.87 examples/s]\n",
      "Generating train split: 20 examples [00:00, 7011.54 examples/s]\n",
      "Formatting comparisons with prompt template: 100%|██████████| 20/20 [00:00<00:00, 2037.31 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 432.38 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m pipeline = SteeringPipeline(\n\u001b[32m      2\u001b[39m     lazy_init=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m      3\u001b[39m     controls=[sppo]\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43msteer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/aisteer360/algorithms/core/steering_pipeline.py:155\u001b[39m, in \u001b[36mSteeringPipeline.steer\u001b[39m\u001b[34m(self, **steer_kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m steer_fn = \u001b[38;5;28mgetattr\u001b[39m(control, \u001b[33m\"\u001b[39m\u001b[33msteer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(steer_fn):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     maybe_new_model = \u001b[43msteer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msteer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_new_model, nn.Module):\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = maybe_new_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/aisteer360/algorithms/structural_control/wrappers/trl/sppotrainer/base_mixin.py:127\u001b[39m, in \u001b[36mSPPOTrainerMixin.steer\u001b[39m\u001b[34m(self, model, tokenizer, ref_model, **_)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# train one iteration\u001b[39;00m\n\u001b[32m    113\u001b[39m trainer = SPPOTrainer(\n\u001b[32m    114\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    115\u001b[39m     ref_model=\u001b[38;5;28mself\u001b[39m.ref_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m     loss_type=training_config.loss_type,\n\u001b[32m    126\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.model = trainer.model\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# save iteration checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:563\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     current_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:535\u001b[39m, in \u001b[36mDPODataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    532\u001b[39m             dtype = torch.int64\n\u001b[32m    534\u001b[39m         \u001b[38;5;66;03m# Convert to tensor and pad\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         to_pad = \u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    536\u001b[39m         padded_batch[k] = pad(to_pad, padding_value=padding_value, padding_side=padding_side)\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m k.endswith(\u001b[33m\"\u001b[39m\u001b[33m_logps\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    538\u001b[39m     \u001b[38;5;66;03m# the cached reference model logprobs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:535\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    532\u001b[39m             dtype = torch.int64\n\u001b[32m    534\u001b[39m         \u001b[38;5;66;03m# Convert to tensor and pad\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         to_pad = [\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m    536\u001b[39m         padded_batch[k] = pad(to_pad, padding_value=padding_value, padding_side=padding_side)\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m k.endswith(\u001b[33m\"\u001b[39m\u001b[33m_logps\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    538\u001b[39m     \u001b[38;5;66;03m# the cached reference model logprobs\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "pipeline = SteeringPipeline(\n",
    "    lazy_init=True, \n",
    "    controls=[sppo]\n",
    ")\n",
    "pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short, constructive response to: 'My partner always interrupts me.'\"\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef0904",
   "metadata": {},
   "source": [
    "## Full-parameter SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8cd3f",
   "metadata": {},
   "source": [
    "Lastly, to run a full-weight fine-tune set `use_peft=False`, drop the LoRA arguments, and usually shrink the batch size (because every parameter now receives gradients). \n",
    "\n",
    "Note: Full fine-tuning can be 10-20 times more memory-intensive than LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sft = SFT(\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "    train_dataset=sft_train,\n",
    "    use_peft=False,               # full FT\n",
    "    output_dir=\"./tmp/sft_full\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    report_to=\"none\",\n",
    "    seed=7,\n",
    ")\n",
    "full_pipeline = SteeringPipeline(\n",
    "    lazy_init=True, \n",
    "    controls=[full_sft]\n",
    ")\n",
    "full_pipeline.steer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba56bdf",
   "metadata": {},
   "source": [
    "The wrapper also provides functionality for resuming training if interrupted (via TRL's `resume_from_checkpoint`) by providing either the directory path of the checkpoint name in `output_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed78815",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_sft = SFT(\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "    train_dataset=sft_train,\n",
    "    output_dir=\"./tmp/sft_lora\",\n",
    "    resume_from_checkpoint=\"./tmp/sft_lora/checkpoint-1000\",\n",
    "    use_peft=True,\n",
    "    adapter_name=\"sft\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "resume_pipeline = SteeringPipeline(\n",
    "    lazy_init=True, \n",
    "    controls=[resume_sft]\n",
    ")\n",
    "resume_pipeline.steer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e20bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
