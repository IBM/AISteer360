{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47dafb6",
   "metadata": {},
   "source": [
    "![SASA method](https://raw.githubusercontent.com/IBM/AISteer360/main/notebooks/images/sasa.png)\n",
    "\n",
    "(Image from Ko et al., 2025)\n",
    "\n",
    "## Large language models can become strong self-detoxifiers\n",
    "\n",
    "**Authors**: Ching-Yun Ko, Pin-Yu Chen, Payel Das, Youssef Mroueh, Soham Dan, Georgios Kollias, Subhajit Chaudhury, Tejaswini Pedapati, Luca Daniel\n",
    "\n",
    "* Paper: https://openreview.net/pdf?id=jY5oml9fe9\n",
    "\n",
    "SASA is an output steering method, enabling the users to perform controlled decoding given any desirable value attributes. \n",
    "\n",
    "SASA leverages the contextual representations from an LLM to learn linear subspaces from labeled data, e.g. characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. \n",
    "\n",
    "In this demo, we show how SASA can be used to reduce the toxicity of sentences generated by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deced0ae",
   "metadata": {},
   "source": [
    "### Method parameters\n",
    "\n",
    "| parameter           | type            | description                                                                                                           |\n",
    "| ------------------- | --------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| `beta`              | `float`         | Scaling coefficient for value redistribution. Must be non-negative.                                                   |\n",
    "| `wv_path`           | `Optional[str]` | Path to a saved steering-vector tensor. Must end with `.pt` if provided.                                              |\n",
    "| `gen_wv_data_path`  | `Optional[str]` | Path to the value dataset, e.g. sentences with labeled toxicity.                                                      |\n",
    "| `gen_wv_length`     | `Optional[int]` | Maximum number of samples used for preparing SASA steering if `wv_path` does not exist.                               |\n",
    "| `gen_wv_batch_size` | `Optional[int]` | Batch size used for preparing SASA steering if `wv_path` does not exist. Must be non-negative if `wv_path` is `None`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a4efe",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (even after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de9fc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /dccstor/larimar/irene/poetry-venvs/aisteer360-9MjJc3W9-py3.12/lib/python3.12/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2e967",
   "metadata": {},
   "source": [
    "## Example: Steering for reduced toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28570abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.output_control.sasa.control import SASA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b6979",
   "metadata": {},
   "source": [
    "### Creating the control\n",
    "\n",
    "We initialize the SASA method with specified parameters. \n",
    "\n",
    "Below, `beta` represents the steering strength with `0` being the original decoding. In the reference paper, authors have tried `beta` as big as `500`. \n",
    "\n",
    "SASA requires contructing the value subspace prior to the steering. To prepare the subspace, users should specify the sample budget `gen_wv_length` for the step. By setting `gen_wv_length = 1000`, users ask to construct the subspace from only 1k samples. By default, the algorithm uses all samples available with `gen_wv_length = -1`. \n",
    "\n",
    "`gen_wv_batch_size` represents the batch size used during this step. Users may also adjust it according to their computational resources.\n",
    "\n",
    "**Note**: You can adjust `beta`, `gen_wv_length`, `gen_wv_batch_size` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa = SASA(\n",
    "    beta=10,\n",
    "    gen_wv_length=100,\n",
    "    gen_wv_batch_size=8,\n",
    "    gen_wv_data_path=\"../../Jigsaw_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb1a27",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "\n",
    "By default, the toxicity subspace is constructed using the Jigsaw dataset from Kaggle. To use `jigsaw_unintended_bias` you can either download it manually from Kaggle (https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) or uncomment and run the following cell using the Kaggle API (https://www.kaggle.com/docs/api). Either way, all files should be extracted to one folder, e.g. `'./tmp/Jigsaw_data/all_data.csv'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae96a7e",
   "metadata": {},
   "source": [
    "#### Automated download instructions (optional)\n",
    "\n",
    "To access your Kaggle token (for downloading data using the API tool), first sign in at [kaggle.com](https://www.kaggle.com). Then:\n",
    "- Click your profile photo -> \"Your Profile\" -> \"Settings\"\n",
    "- Scroll to API and click \"Create New Token\"\n",
    "- Your browser immediately downloads `kaggle.json`\n",
    "\n",
    "Place the json in the kaggle directory in root (typically `~/.config/kaggle/`) and execute the following script. \n",
    "\n",
    "**Note**: If you encounter an error 403 (permission error), please ensure that you have clicked \"Join the competition\" under the \"Data\" tab on the dataset homepage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b145f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m ensurepip --upgrade\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, zipfile, shutil, pandas as pd\n",
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "DATA_DIR = Path(\"tmp/Jigsaw_data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = KaggleApi(); api.authenticate()\n",
    "api.competition_download_files(\n",
    "    \"jigsaw-unintended-bias-in-toxicity-classification\",\n",
    "    path=str(DATA_DIR),\n",
    "    force=True,\n",
    "    quiet=False\n",
    ")\n",
    "\n",
    "zip_path = glob.glob(str(DATA_DIR / \"*.zip\"))[0]\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    z.extractall(DATA_DIR)\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "label_paths = [\n",
    "    p for p in (\n",
    "        DATA_DIR / \"test_public_expanded.csv\",\n",
    "        DATA_DIR / \"test_private_expanded.csv\",\n",
    "        DATA_DIR / \"test_labels.csv\"\n",
    "    ) if p.exists()\n",
    "]\n",
    "if label_paths:\n",
    "    lbl = pd.concat([pd.read_csv(p) for p in label_paths])\n",
    "    test = test.merge(lbl[[\"id\", \"toxicity\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "out_csv = DATA_DIR / \"all_data.csv\"\n",
    "pd.concat([train, test]).to_csv(out_csv, index=False)\n",
    "\n",
    "# cleanup\n",
    "os.remove(zip_path)\n",
    "for p in DATA_DIR.iterdir():\n",
    "    if p.resolve() != out_csv.resolve():\n",
    "        (p.unlink() if p.is_file() else shutil.rmtree(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5497005",
   "metadata": {},
   "source": [
    "If value subspace is available, users can skip these parameters and pass the path to the subspace via `wv_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4d08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa = SASA(\n",
    "    beta=10,\n",
    "    wv_path='./steer_wv.pt',  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28dc65",
   "metadata": {},
   "source": [
    "### Creating the steering pipeline\n",
    "\n",
    "We create a `SteeringPipeline` with the `SASA` control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f0d20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca07e1d61e2b473e8e66e41ed99571d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sasa_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[sasa],\n",
    "    device_map=\"cuda\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d80d96",
   "metadata": {},
   "source": [
    "Next we steer the pipeline (under the single SASA control). Note that since we have initialized the SASA control with the path to the toxicity data, as opposed to passing in a trained subspace, steering requires learning this subspace from the data. This is resource-heavy step (GPU required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7426e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SASA steer (wv)......\n"
     ]
    }
   ],
   "source": [
    "sasa_pipeline.steer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cf2cc",
   "metadata": {},
   "source": [
    "After steering, inference can now be run on the pipeline for a given prompt. We define a prompt that attempts to induce toxic behavior in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f035bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"I hate you. You are being like a drama queen who seeks attention all the time. I wish you\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac936f",
   "metadata": {},
   "source": [
    "Passing this prompt into the unsteered (base) model demonstrates a toxic continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a0f24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073ce15096d8426faf7222b4a47a1151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response (baseline):\n",
      "\n",
      " would just be normal for once.\n",
      "You may feel angry or upset with your partner, but calling them names and telling them how you feel is not the\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "gen_params = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "baseline_outputs = model.generate(\n",
    "    **inputs.to(model.device), \n",
    "    **gen_params\n",
    ")\n",
    "\n",
    "# BASELINE GNEERATE CODE\n",
    "print(f\"\\nResponse (baseline):\\n\")\n",
    "print(tokenizer.decode(baseline_outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27bbe7",
   "metadata": {},
   "source": [
    "Compare this with the response of the base model when steered using SASA (via the steering pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932882b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response (SASA):\n",
      "\n",
      " would just calm down and talk to me normally. I don’t want to fight with you. I want us to get along. I want to have\n"
     ]
    }
   ],
   "source": [
    "steered_output_ids = sasa_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse (SASA):\\n\")\n",
    "print(tokenizer.decode(steered_output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828201b3",
   "metadata": {},
   "source": [
    "Lastly, note that the beta parameter dictates the strength of the steering, and can thus be adjusted to control the degree of toxicity suppression in the generated response (importantly without having to relearn the subspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3095cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa = SASA(\n",
    "    beta=0,\n",
    "    wv_path='./steer_wv.pt',  # we just saved the subspace in the preparation steps above\n",
    ")\n",
    "\n",
    "sasa_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[sasa],\n",
    "    device_map=\"cpu\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")\n",
    "\n",
    "sasa_pipeline.steer()\n",
    "\n",
    "original_output_ids = sasa_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "original_response = tokenizer.decode(original_output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Steered response: {original_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
