{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39909f1-f4a5-4675-ba2f-ec703e629456",
   "metadata": {},
   "source": [
    "## Running TRL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a9fe4-118e-4e86-823c-f0543bc442e8",
   "metadata": {},
   "source": [
    "The toolkit implements some of the [TRL](https://github.com/huggingface/trl) methods via a `StructuralControl` wrapper. The methods currently available are `Supervised Fine-Tuning` (SFT), `Direct Preference Optimization` (DPO), and `Anchored Preference Optimization` (APO).\n",
    "\n",
    "The toolkit provides another preference optimization method, `Self-Play Preference Optimization` ([SPPO](https://github.com/uclaml/SPPO)), that is not available in the TRL library but follows the design of the TRL methods very closely. \n",
    "\n",
    "The TRL methods are implemented via a `StructuralControl` wrapper. Methods are initiated using a `Control` object. This notebook demonstrates how the above methods can be used for training languae models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed2939d-fc3c-4eba-9fc4-2f419e19a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c4488-3948-453d-a32c-8ba4eea509ae",
   "metadata": {},
   "source": [
    "### SFT with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebfddd-213a-42ef-963c-d13245bc0904",
   "metadata": {},
   "source": [
    "To run SFT, we need to import SteeringPipeline as well as the SFT control class. The method is implemented as a wrapper around TRL's SFTTrainer class and as such is used similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d15b3d-8daf-4afc-a379-4bc40ad0ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.sfttrainer.control import SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432c5bc-74a7-40c4-9c40-c689c61ad075",
   "metadata": {},
   "source": [
    "The example shows supervised fine tuning of a small model with a 500 record sample of a Huggingface preference dataset. We load the tokenizer and preprocess the dataset to convert it to a standard format for SFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc3b7c6-f7f6-40c0-a769-d07ab2265d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['chosen']}\"\n",
    "    tok_data =  tokenizer(text, truncation=True, padding='max_length', max_length=1024, return_tensors=\"pt\")\n",
    "    return {'input_ids': tok_data['input_ids'][0], 'attention_mask': tok_data['attention_mask'][0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfb7d46-fa50-4363-b75a-0fa3a1e2bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfdb92a4-e54d-4d58-a1d4-d2278bdd01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    'HuggingFaceH4/ultrafeedback_binarized',\n",
    "    split='train_prefs',\n",
    ")\n",
    "\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))\n",
    "train_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebf669-3c68-4b5c-8872-43aa632dacfa",
   "metadata": {},
   "source": [
    "Next, the SFT control is instantiated by providing the `train_dataset` as well as the `output_dir` for saving the steered model. We also set `use_peft` to True (default is False) and set `peft_type` to enable LoRA. Finally, we override some of the default training arguments. Note that SFT control is based on TRL's `SFTConfig` class and uses the default training arguments from there. However, some of these parameters can be ovverriden, as shown below. Please refer to `aisteer360.algorithms.structural_control.wrappers.trl.args.py` and `aisteer360.algorithms.structural_control.wrappers.trl.sfttrainer.args.py` to see the list of these parameters and their default values. The parameters used for LoRA training are similarly based on the `LoraConfig` class, and default values can be overriden as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59e732b-c5bc-46d7-86aa-24eeb67b1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftType\n",
    "\n",
    "# control\n",
    "sft = SFT(\n",
    "    train_dataset=train_dataset,\n",
    "    use_peft=True,\n",
    "    peft_type=PeftType.LORA,\n",
    "\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"Qwen2.5-0.5B-SFT-LoRA-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "\n",
    "        \"lora_alpha\": 16,\n",
    "    },\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee11134-a090-4adc-af38-ac09f1eb2102",
   "metadata": {},
   "source": [
    "We then create the SteeringPipeline, providing it the `model_name_or_path`, set the control to `sft` and invoke `steer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8896d9b-c992-4b14-9ddc-b566644b607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering pipeline\n",
    "sft_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=model_name,\n",
    "    controls=[sft],\n",
    "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c75a5b5-21b7-4c37-86cd-31637c968578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_collator is  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.657700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300c9ced-c387-4442-ace0-66ce80148a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:In this task, you are given a second sentence. Your task is to generate the first sentence on the same topic but incoherent and inconsistent with the second sentence.\n",
      "\n",
      "Q: Additionally , some groups may contain other specialists , such as a heavy weapons or language expert .\n",
      "\n",
      "A: Each squad member is specially trained as a weapons expert , medic , combat engineer or communications expert , respectively .\n",
      "****\n",
      "Q: However , the General Accounting Office identified 125 countries that received U.S. training and assistance for their police forces during fiscal year 1990 at a cost of at least $117 million .\n",
      "\n",
      "A: No government agency is in charge of calculating the cost .\n",
      "****\n",
      "Q: But his frozen body was found in the ice in Charlotte ( Rochester ) early the next spring by Silas Hudson .\n",
      "\n",
      "A:\n",
      "output (SFT):\n",
      "[' The man who had been found dead in the freezing ice in Charlotte ( Rochester ) was named Silas']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    'HuggingFaceH4/ultrafeedback_binarized',\n",
    "    split='test_prefs',\n",
    ")\n",
    "enc = tokenizer(f\"Question:{dataset[0]['prompt']} \\n Answer:\", return_tensors=\"pt\", padding=True).to(sft_pipeline.model.device)\n",
    "print(f\"Question:{dataset[0]['prompt']}\")\n",
    "steered_response = sft_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=20\n",
    ")\n",
    "print(\"output (SFT):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c893960-7f40-48ae-b40f-ac65b77d7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "import gc\n",
    "del sft_pipeline.model, sft_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb819e-cac8-4980-b49e-b37d66408313",
   "metadata": {},
   "source": [
    "We load the LoRA adapter, merge it into the base model, and save the combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dafd13b6-6e9b-4a09-abde-70762391a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load PEFT config\n",
      "# Load base model\n",
      "# Get PeftModel\n",
      "# Merge adapter into model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/tokenizer_config.json',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/special_tokens_map.json',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/chat_template.jinja',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/vocab.json',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/merges.txt',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/added_tokens.json',\n",
       " 'Qwen2.5-0.5B-SFT-LoRA-Steer-Merged/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "lora_adapter_path = \"Qwen2.5-0.5B-SFT-LoRA-Steer\"\n",
    "\n",
    "print('# Load PEFT config')\n",
    "config = PeftConfig.from_pretrained(lora_adapter_path)\n",
    "\n",
    "print('# Load base model')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_adapter_path)\n",
    "\n",
    "print('# Get PeftModel')\n",
    "peft_model = PeftModel.from_pretrained(base_model, lora_adapter_path, 'abcd')\n",
    "\n",
    "breakpoint()\n",
    "peft_model.set_adapter('abcd')  # set adapter as active\n",
    "\n",
    "print(\"# Merge adapter into model\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "breakpoint()\n",
    "# merged_model.save_pretrained(\"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")\n",
    "merged_model.save_pretrained(\"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")\n",
    "tokenizer.save_pretrained(\"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4a08a8-6e8c-4a46-942e-f5436e26ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_model, tokenizer, peft_model, merged_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15163a06-c5b5-4db8-9a2e-4a46437c7b88",
   "metadata": {},
   "source": [
    "### DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fccec-c1fa-4eb3-8d16-9eaa046abbc4",
   "metadata": {},
   "source": [
    "We next further steer the above SFT LoRA model using DPO.\n",
    "\n",
    "In the example below, we use a preference dataset that is already in a conversational format needed by DPO so no preprocessing is neeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81782e73-0a60-4a51-9f63-817dfebf3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9eb6b6-1481-4efe-9f3d-0ce161569044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e73de55f-834b-4719-8a22-6a98e69f0d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': [{'content': 'Use the pygame library to write a version of the classic game Snake, with a unique twist',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Sure, I'd be happy to help you write a version of the classic game Snake using the pygame library! Here's a basic outline of how we can approach this:\\n\\n1. First, we'll need to set up the game display and create a game object that we can use to handle the game's state.\\n2. Next, we'll create the game's grid, which will be used to represent the game board. We'll need to define the size of the grid and the spaces within it.\\n3. After that, we'll create the snake object, which will be used to represent the player's movement. We'll need to define the size of the snake and the speed at which it moves.\\n4. We'll also need to create a food object, which will be used to represent the food that the player must collect to score points. We'll need to define the location of the food and the speed at which it moves.\\n5. Once we have these objects set up, we can start handling the game's logic. We'll need to define the rules for the player's movements, such as how the player can move the snake and how the snake will grow as the player collects more food.\\n6. We'll also need to add collisions detection to the game, so that the snake and the food can collide with each other.\\n7. Finally, we'll need to add a user interface to the game, such as a menu and a scoreboard.\\n\\nNow, as for the unique twist, we could add a few features to make the game more interesting. For example, we could add power-ups that give the player special abilities, such as the ability to grow the snake faster or to change its direction. We could also add obstacles, such as walls or pits, that the player must avoid.\\n\\nHere's some sample code to get us started:\\n```\\nimport pygame\\n\\n# Initialize pygame\\npygame.init()\\n\\n# Set up the game display\\nwidth = 800\\nheight = 600\\nscreen = pygame.display.set_mode((width, height))\\n\\n# Define the colors\\nWHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\nGREEN = (0, 255, 0)\\n\\n# Define the game objects\\nsnake = pygame.Rect(50, 50, 10, 10)\\nfood = pygame.Rect(100, 100, 10, 10)\\n\\n# Define the game logic\\ndef update_snake():\\n    # Get the mouse position\\n    mouse_pos = pygame.mouse.get_pos()\\n\\n    # Move the snake based on the mouse position\\n    if mouse_pos[0] > snake.x:\\n        snake.x += 10\\n    elif mouse_pos[0] < snake.x:\\n        snake.x -= 10\\n    elif mouse_pos[1] > snake.y:\\n        snake.y += 10\\n    elif mouse_pos[1] < snake.y:\\n        snake.y -= 10\\n\\n    # Update the snake's size\\n    if snake.x == food.x and snake.y == food.y:\\n        snake.width += 10\\n        snake.height += 10\\n\\n# Define the game loop\\ndef game_loop():\\n    # Clear the screen\\n    screen.fill(BLACK)\\n\\n    # Update the game objects\\n    update_snake()\\n\\n    # Draw the game objects\\n    screen.fill(WHITE)\\n    screen.draw.rect(snake, GREEN)\\n    screen.draw.rect(food, BLACK)\\n\\n    # Update the display\\n    pygame.display.flip()\\n\\n# Start the game loop\\ngame_loop()\\n\\n# Run the game\\nwhile True:\\n    for event in pygame.event.get():\\n        if event.type == pygame.QUIT:\\n            pygame.quit()\\n            break\\n\\n    pygame.time.Clock().tick(60)\\n```\\nThis code sets up a basic game display, defines the snake and food objects, and updates the game state based on the player's movements. We've also added a simple user interface and some basic collision detection.\\n\\nAs for the unique twist, we could add a few features to make the game\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected': [{'content': 'Use the pygame library to write a version of the classic game Snake, with a unique twist',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Sure, here\\'s an example of how to write a version of Snake game with a unique twist using the Pygame library:\\n```python\\nimport pygame\\n\\nclass SnakeGame:\\n    def __init__(self, game_width, game_height):\\n        pygame.init()\\n        screen = pygame.display.set_mode((game_width, game_height))\\n        pygame.display.set_caption(\"Snake Game\")\\n        self.speed = 5  # Speed of the snake\\n        self.food_speed = 1  # Speed of the food\\n        self.direction = 0  # Initial direction of the snake\\n        self.snakelen = 0  # Length of the snake\\n        self.food = pygame.image.load(\"snake_food.png\")\\n        self.head = pygame.image.load(\"snake_head.png\")\\n        self.tail = pygame.image.load(\"snake_tail.png\")\\n        self.game Quint()\\n    def Quint(self):\\n        for i in range(50):\\n            pygame.draw.line(screen, (180, 100, 220), (0, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 300), 2)\\n            pygame.display.flip()\\n        self.game.run()\\n    def run(self):\\n        while True:\\n            for event in pygame.event. pygame.KEYDOWN:\\n                if event.key == pygame.K_LEFT:\\n                    self.direction = -1\\n                if event.key == pygame.K_RIGHT:\\n                    self.direction = 1\\n            self.snakelen += 1\\n            if self.snakelen == 0:\\n                self.snakelen = 10\\n            if self.snakelen > 20:\\n                self.snakelen = 20\\n            self.gameQuint()\\n            self.foodCrossing()\\n            self.headRun()\\n            pygame.display.update()\\ngame = SnakeGame(800, 600)\\ngame.run()\\n```\\nIn this game, the snake moves with a constant speed, but the direction of the snake can be controlled by the user using the left and right arrow keys. The snake grows in length every 10 segments, and when it reaches a certain length, it resets to 10 segments. The food moves fast and randomly crosses the screen, and the snake can eat it by colliding with it. The snake\\'s head and tail move independently of each other. The game ends when the snake dies or reaches the end of the screen.',\n",
       "   'role': 'assistant'}],\n",
       " 'score_chosen': 6.0,\n",
       " 'score_rejected': 4.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cdf6b-4f0a-4f5a-a6d2-b184b20cdfcb",
   "metadata": {},
   "source": [
    "To use DPO, we import the corresponding DPO control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a0a73a-a3ba-4853-9c22-40900b68bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ecae8-d5b7-434a-bdb5-ed7b75d75627",
   "metadata": {},
   "source": [
    "DPO steering is run the same way as SFT above. The DPO control is created and steering pipeline in invoked after providing the model name and control set to `dpo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a35fe7-53ab-4214-a131-c59cf36c59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "dpo = DPO(\n",
    "    train_dataset=dataset,\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"Qwen2.5-0.5B-DPO-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a7da81-8f6b-4c7d-9d44-9df5ae831e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering pipeline\n",
    "dpo_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=model_name,\n",
    "    controls=[dpo],\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a54d4d7-7396-46a8-87e5-f9c1f3996ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec14a4cb-214e-4db4-848f-e6a1edaf1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESIONAs an HR manager, you want to test a potential employee's ability to solve puzzles to determine their suitability for a job. Write a Python script that generates a list of questions that require logical reasoning to answer. Your list should include questions related to mathematical puzzles, language puzzles, logic puzzles, lateral thinking puzzles, and pattern recognition puzzles. Use the following code as a starting point:\n",
      "questions = {\n",
      "    \"Mathematical puzzles\": [\"If the value of x+y = 20 and x-y = 10, what is the value of x and y?\", \"If a pizza has a radius of 8 inches and is cut into 6 equal slices, what is the area of each slice?\"],\n",
      "    \"Language puzzles\": [\"What word starts with 'e' and ends with 'e' but only contains one letter?\", \"I am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person. What am I?\"],\n",
      "    \"Logic puzzles\": [\"You have 3 boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. The boxes have been incorrectly labeled such that no label identifies the actual contents of the box it labels. Opening just one box, and without looking in the box, you take out one piece of fruit. From that one piece of fruit, how can you immediately label all of the boxes correctly?\"],\n",
      "    \"Lateral thinking puzzles\": [\"A man lives on the 10th floor of a building. Every day he takes the elevator to the ground floor to go to work or to go shopping. When he returns he takes the elevator to the 7th floor and walks up the stairs to reach his apartment on the 10th floor. Why does he do this?\"],\n",
      "    \"Pattern recognition puzzles\": [\"What is the next number in the sequence: 1, 3, 6, 10, 15, ___\", \"What is the missing number in the sequence: 2, 5, 10, 17, ___, 37\"]\n",
      "}\n",
      "for category in questions:\n",
      "    print(f\"{category}:\")\n",
      "    for question in questions[category]:\n",
      "        print(f\"- {question}\") \n",
      "Run the script and use the list of questions to conduct a comprehensive interview with the applicant. Their ability to solve puzzles will help you determine their suitability for the job.\n",
      "output (DPO):\n",
      "['Here\\'s the Python script based on your request:\\n```python\\nimport random\\n\\n# Generate a list of math, lanugage, logic, and lterature puzzle questions\\nquestions = [\\n    {\"category\": \"Mathematical puzzles\", \"question\": \"If the value of x + y = 20 and x - y = 10, what is the value of x and y?\"},\\n    {\"category\": \"Language puzzles\", \"question\": \"What word starts with \\'']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\")\n",
    "question = 'QUESION'+dataset[0]['chosen'][-2]['content'].rsplit('QUESTION',1)[-1]\n",
    "print(question)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "enc = tokenizer(text, return_tensors=\"pt\", padding=True, padding_side=\"left\").to(dpo_pipeline.model.device)\n",
    "steered_response = dpo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (DPO):\")\n",
    "print(steered_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85de4b09-6b95-4a94-b3bf-ccfce57392f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "import gc\n",
    "del dpo_pipeline.model, dpo_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13defcb8-8bcc-4759-b7d1-b9349750b545",
   "metadata": {},
   "source": [
    "### APO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d593228-4bc2-4a31-afc3-fa4f02f7eb3d",
   "metadata": {},
   "source": [
    "Now, we demonstrate how to run APO with the same previously steered SFT LoRA model. APO is run in the same manner as DPO above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af06ade-528f-4d60-a1a2-cf078eae3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062ee6cb-a45d-4bbb-89c6-4f0cbed2573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f02bc1-c11e-4d4a-84a1-303c9656dc15",
   "metadata": {},
   "source": [
    "We use the APO control and set the SteeringPipeline with APO as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ff50f1-75cb-4169-ac5c-a1f19bcf45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.apotrainer.control import APO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06fd44a3-b8db-4b0f-9d10-a2e748864729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "apo = APO(\n",
    "    train_dataset=dataset,\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"Qwen2.5-0.5B-APO-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a64f558a-da6d-491f-bbcb-441640b0d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering pipeline\n",
    "apo_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=model_name,\n",
    "    controls=[apo],\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12ce47ff-e0fc-46e2-b414-f6c261e47d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apo_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6880c69-c636-4368-8c5f-35195081e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESIONAs an HR manager, you want to test a potential employee's ability to solve puzzles to determine their suitability for a job. Write a Python script that generates a list of questions that require logical reasoning to answer. Your list should include questions related to mathematical puzzles, language puzzles, logic puzzles, lateral thinking puzzles, and pattern recognition puzzles. Use the following code as a starting point:\n",
      "questions = {\n",
      "    \"Mathematical puzzles\": [\"If the value of x+y = 20 and x-y = 10, what is the value of x and y?\", \"If a pizza has a radius of 8 inches and is cut into 6 equal slices, what is the area of each slice?\"],\n",
      "    \"Language puzzles\": [\"What word starts with 'e' and ends with 'e' but only contains one letter?\", \"I am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person. What am I?\"],\n",
      "    \"Logic puzzles\": [\"You have 3 boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. The boxes have been incorrectly labeled such that no label identifies the actual contents of the box it labels. Opening just one box, and without looking in the box, you take out one piece of fruit. From that one piece of fruit, how can you immediately label all of the boxes correctly?\"],\n",
      "    \"Lateral thinking puzzles\": [\"A man lives on the 10th floor of a building. Every day he takes the elevator to the ground floor to go to work or to go shopping. When he returns he takes the elevator to the 7th floor and walks up the stairs to reach his apartment on the 10th floor. Why does he do this?\"],\n",
      "    \"Pattern recognition puzzles\": [\"What is the next number in the sequence: 1, 3, 6, 10, 15, ___\", \"What is the missing number in the sequence: 2, 5, 10, 17, ___, 37\"]\n",
      "}\n",
      "for category in questions:\n",
      "    print(f\"{category}:\")\n",
      "    for question in questions[category]:\n",
      "        print(f\"- {question}\") \n",
      "Run the script and use the list of questions to conduct a comprehensive interview with the applicant. Their ability to solve puzzles will help you determine their suitability for the job.\n",
      "output (APO):\n",
      "['Sure! Here\\'s the Python script that uses the provided questions to generate puzzle-related questions:\\n\\n```python\\nimport random\\n\\n# List of questions categorized by type\\nquestions = {\\n    \"Mathematical puzzles\": [\"If the value of x+y = 20 and x-y = 10, what is the value of x and y?\", \"If a pizza has a radius of 8 inches and is cut into 6 equal slices, what is the area of each slice?\"],\\n   ']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\")\n",
    "question = 'QUESION'+dataset[0]['chosen'][-2]['content'].rsplit('QUESTION',1)[-1]\n",
    "print(question)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "enc = tokenizer(text, return_tensors=\"pt\", padding=True, padding_side=\"left\").to(apo_pipeline.model.device)\n",
    "steered_response = apo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (APO):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c31beee-daf8-43dd-8b38-eda5dbc37555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "del apo_pipeline.model, apo_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413397d-fa69-41a5-a36f-8331b6376c9a",
   "metadata": {},
   "source": [
    "### SPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e778e3-015f-42a6-9c05-7fa00184424b",
   "metadata": {},
   "source": [
    "To run SPPO, extra classes need to be imported, and multiple iterations of steering can be performed. The example below is based on the [SPPO paper](https://arxiv.org/abs/2405.00675) and the iteration code below is based on scripts from the [SPPO github repository](https://github.com/uclaml/SPPO/tree/main).\n",
    "\n",
    "The example shows 3 iterations of SPPO applied to a Mistral model using a Huggingface prompt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e5d5fb-419d-4e1c-92ae-23467df0602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.sppotrainer.control import SPPO\n",
    "\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.sppotrainer.utils import (\n",
    "    set_seed,\n",
    "    apply_template,\n",
    "    ranking,\n",
    "    from_ranks,\n",
    "    prepare_score,\n",
    "    apply_chat_template,\n",
    "    process_dataset,\n",
    "    prepare_dataset_from_prompts\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d642691b-a6b1-41a7-8996-6a8bc8f4f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SPPO(to_be_steered_model_path_or_name, data, sppo_temp_dir, start_iter_num=1, end_iter_num=1, maxlen = 2048, \n",
    "             num_prompts=5, additional_train_datasets=None):\n",
    "    checkpoints_path = \"\"\n",
    "    steerer = None\n",
    "\n",
    "    checkpoints_path=f\"{sppo_temp_dir}/checkpoints/SPPO-FINAL\"  #steered model stored at each iteration\n",
    "          \n",
    "\n",
    "    # Steer model\n",
    "    sppo = SPPO(\n",
    "        train_dataset=data,\n",
    "        eval_dataset=None,\n",
    "        **{\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"num_train_epochs\": 1,\n",
    "            \"learning_rate\": 5.0e-7,\n",
    "            \"output_dir\": checkpoints_path,\n",
    "            \"save_strategy\": \"no\",\n",
    "            \"beta\": 0.001,\n",
    "            \"optim\": \"rmsprop\",\n",
    "            \"loss_type\": \"sppo\",\n",
    "            \"max_prompt_length\": 128,\n",
    "            \"max_length\": 512\n",
    "    \n",
    "        },\n",
    "    )\n",
    "\n",
    "    # steerer\n",
    "    sppo_pipeline = SteeringPipeline(\n",
    "        model_name_or_path=to_be_steered_model_path_or_name,\n",
    "        controls=[sppo],\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "        hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    "    )\n",
    "\n",
    "    sppo_pipeline.steer(num_prompts=num_prompts, start_iter_num=start_iter_num, end_iter_num=end_iter_num, \n",
    "                                  additional_train_datasets=additional_train_datasets, sppo_temp_dir=sppo_temp_dir, maxlen=maxlen)\n",
    "\n",
    "    return sppo_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ed07cca-cdcb-48d7-9de9-88c26b58a06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4227f9e556d54cd393e405bc35e00705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /dccstor/modelaudit/users/moninder/cache/hub/llm-blender/PairRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.24it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4064ff48c814aec89d74d16917547d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter1_score/train.parquet\n",
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter1_score/test.parquet\n",
      "probs calculated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8232f929fab44bf891397f1cee69da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f01621206e4353a675571eda722a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3646651a375d4e36adbaec2def735a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>62423.399500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /dccstor/modelaudit/users/moninder/cache/hub/llm-blender/PairRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.62it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5009dd3c8e924a83ab4c377ea794c84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter2_score/train.parquet\n",
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter2_score/test.parquet\n",
      "probs calculated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6730a5975f1448c5832f666cd130f0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e1788d7b9f41be9721e2edec9fd9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755a2d3a72ff434a81159371e4ca7738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>60369.968500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/dccstor/aimetacognition/users/moninder/AISteer360/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /dccstor/modelaudit/users/moninder/cache/hub/llm-blender/PairRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.21it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca298eedb0e489a96efd0539c06e5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter3_score/train.parquet\n",
      "Saved file to Mistral-7B-Instruct-v0.2_SPPO/synthetic_data_SPPO-Iter3_score/test.parquet\n",
      "probs calculated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896c78fc728f476694ebd1703325037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b34c09585746eba9f055f10e801f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68c422b7f374a14a566abacaab6f5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38179.032200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Based on https://github.com/uclaml/SPPO/blob/main/run_sppo_mistral.sh\n",
    "\n",
    "start_iter_num = 1\n",
    "end_iter_num = 3\n",
    "num_prompts = 2  # number of responses to generate for each prompt (default is 5)\n",
    "\n",
    "\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"Qwen/Qwen2.5-0.5B-Instruct\" #\n",
    "\n",
    "\n",
    "prompt_datasets=[\"UCLA-AGI/data-mistral-7b-instruct-sppo-iter1\", \n",
    "                 \"UCLA-AGI/data-mistral-7b-instruct-sppo-iter2\", \n",
    "                 \"UCLA-AGI/data-mistral-7b-instruct-sppo-iter3\" ] #prompt datasets to be used\n",
    "\n",
    "\n",
    "m_name = BASE_MODEL.split(\"/\")[-1]\n",
    "sppo_temp_dir = m_name+\"_SPPO\"\n",
    "\n",
    "# We use just 10 records of each dataset for the demonstration\n",
    "subset_size = 10\n",
    "\n",
    "dataset = load_dataset(prompt_datasets[start_iter_num-1], split=\"train\")\n",
    "data = dataset.select(list(range(subset_size)))\n",
    "del dataset\n",
    "\n",
    "additional_train_datasets = []\n",
    "for dset in range(start_iter_num, end_iter_num):\n",
    "    dataset = load_dataset(prompt_datasets[dset], split=\"train\")\n",
    "    addl_data = dataset.select(list(range(subset_size)))\n",
    "    additional_train_datasets.append(addl_data)\n",
    "    del dataset\n",
    "\n",
    "\n",
    "\n",
    "if start_iter_num == 1:\n",
    "    to_be_steered_model_path_or_name = BASE_MODEL\n",
    "else:\n",
    "    to_be_steered_model_path_or_name = f\"{sppo_temp_dir}/checkpoints/SPPO-Iter{start_iter_num-1}\"\n",
    "\n",
    "\n",
    "\n",
    "sppo_pipeline = run_SPPO(to_be_steered_model_path_or_name, data=data, sppo_temp_dir=sppo_temp_dir, start_iter_num=start_iter_num, end_iter_num=end_iter_num, \n",
    "         additional_train_datasets=additional_train_datasets, num_prompts=num_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fa49ebf-06b4-4e5b-9d6d-9a9bdf5f4b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Can you write me a 2 minute speech to welcome people to my dim sum themed 36th birthday party? [/INST] \n",
      "output (SPPO):\n",
      "[\"Ladies and Gentlemen, esteemed guests, welcome to my 36th birthday party! I'm so glad you could all join me tonight to celebrate this milestone in my life. I wanted to throw a party that reflected my love for one of my favorite foods - dim sum!\\n\\nDim sum, for those who may not be familiar, is a traditional Chinese culinary art that involves serving small bite-sized portions of food in steamer baskets or on\"]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f\"UCLA-AGI/data-mistral-7b-instruct-sppo-iter1\", split=\"train\")\n",
    "\n",
    "subset_size = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "prompt = apply_template(dataset[subset_size][\"prompt\"], tokenizer)\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\").to(sppo_pipeline.model.device)  \n",
    "\n",
    "steered_response = sppo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (SPPO):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58430539-a6bb-4d51-ae0d-88d612fc4a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
