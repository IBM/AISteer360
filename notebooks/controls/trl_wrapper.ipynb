{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39909f1-f4a5-4675-ba2f-ec703e629456",
   "metadata": {},
   "source": [
    "# Running TRL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a9fe4-118e-4e86-823c-f0543bc442e8",
   "metadata": {},
   "source": [
    "The toolkit implements some of the [TRL](https://github.com/huggingface/trl) methods via a `StructuralControl` wrapper. This guide shows how to run several preference-optimization methods:\n",
    "\n",
    "- SFT (supervised fine-tuning)\n",
    "- DPO (direct preference optimization)\n",
    "- APO (anchored preference optimization).\n",
    "- SPPO (self-play preference optimization)\n",
    "\n",
    "Note that while [SPPO](https://github.com/uclaml/SPPO) is not a part of TRL, it follows many of the similar abstractions so we include it as part of our TRL wrapper.\n",
    "\n",
    "As will be shown, each of the above methods in our toolkit uses the same high-level pattern:\n",
    "- Create a control (e.g., `SFT`, `DPO`, `APO`, `SPPO`) with training args/dataset.\n",
    "- Wrap the control in a `SteeringPipeline`\n",
    "- Call the `steer()` method to run the training logic\n",
    "- Run inference (and optionally merge any adapters into the base model for export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16684a95",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d45b00",
   "metadata": {},
   "source": [
    "If running this from a Google Colab notebook, please uncomment the following cell to install the toolkit. The following block is not necessary if running this notebook from a virtual environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab59d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/AISteer360.git\n",
    "# %cd AISteer360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bcf35",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df80550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install ipywidgets\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2939d-fc3c-4eba-9fc4-2f419e19a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c4488-3948-453d-a32c-8ba4eea509ae",
   "metadata": {},
   "source": [
    "## SFT with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebfddd-213a-42ef-963c-d13245bc0904",
   "metadata": {},
   "source": [
    "We'll SFT on a prompt/answer style corpus.\n",
    "\n",
    "The following example runs SFT, implemented using our wrapper around TRL's `SFTTrainer` class. First, import the `SteeringPipeline` class and the `SFT` control class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d15b3d-8daf-4afc-a379-4bc40ad0ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.sfttrainer.control import SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432c5bc-74a7-40c4-9c40-c689c61ad075",
   "metadata": {},
   "source": [
    "The example shows supervised fine tuning of a small model with a 500 record sample of a Huggingface preference dataset. We load the tokenizer and preprocess the dataset to convert it to a standard format for SFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3b7c6-f7f6-40c0-a769-d07ab2265d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(example):\n",
    "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['chosen']}\"\n",
    "    tok_data =  tokenizer(text, truncation=True, padding='max_length', max_length=1024, return_tensors=\"pt\")\n",
    "    return {\n",
    "        'input_ids': tok_data['input_ids'][0], \n",
    "        'attention_mask': tok_data['attention_mask'][0]\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'HuggingFaceH4/ultrafeedback_binarized',\n",
    "    split='train_prefs',\n",
    ")\n",
    "\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))\n",
    "train_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebf669-3c68-4b5c-8872-43aa632dacfa",
   "metadata": {},
   "source": [
    "Next, the SFT control is instantiated by providing the `train_dataset` as well as the `output_dir` for saving the steered model. We also set `use_peft` to True (default is False) and set `peft_type` to enable LoRA. Finally, we override some of the default training arguments. Note that SFT control is based on TRL's `SFTConfig` class and uses the default training arguments from there. However, some of these parameters can be ovverriden, as shown below. Please refer to `aisteer360.algorithms.structural_control.wrappers.trl.args.py` and `aisteer360.algorithms.structural_control.wrappers.trl.sfttrainer.args.py` to see the list of these parameters and their default values. The parameters used for LoRA training are similarly based on the `LoraConfig` class, and default values can be overriden as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e732b-c5bc-46d7-86aa-24eeb67b1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftType\n",
    "\n",
    "# control\n",
    "sft = SFT(\n",
    "    train_dataset=train_dataset,\n",
    "    use_peft=True,\n",
    "    peft_type=PeftType.LORA,\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"./tmp/Qwen2.5-0.5B-SFT-LoRA-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"lora_alpha\": 16,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee11134-a090-4adc-af38-ac09f1eb2102",
   "metadata": {},
   "source": [
    "We then create the SteeringPipeline, providing it the `model_name_or_path`, set the control to `sft` and invoke `steer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8896d9b-c992-4b14-9ddc-b566644b607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[sft],\n",
    "    device_map=\"auto\",\n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101015e7",
   "metadata": {},
   "source": [
    "We now run the trianing process by calling the steer method of the above SFT pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75a5b5-21b7-4c37-86cd-31637c968578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sft_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c9ced-c387-4442-ace0-66ce80148a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset(\n",
    "    'HuggingFaceH4/ultrafeedback_binarized',\n",
    "    split='test_prefs',\n",
    ")\n",
    "\n",
    "enc = tokenizer(f\"Question:{dataset[0]['prompt']} \\n Answer:\", return_tensors=\"pt\", padding=True).to(sft_pipeline.model.device)\n",
    "print(f\"Question:{dataset[0]['prompt']}\")\n",
    "\n",
    "steered_response = sft_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=20\n",
    ")\n",
    "print(\"output (SFT):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c893960-7f40-48ae-b40f-ac65b77d7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "import gc\n",
    "del sft_pipeline.model, sft_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb819e-cac8-4980-b49e-b37d66408313",
   "metadata": {},
   "source": [
    "We load the LoRA adapter, merge it into the base model, and save the combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd13b6-6e9b-4a09-abde-70762391a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "lora_adapter_path = \"Qwen2.5-0.5B-SFT-LoRA-Steer\"\n",
    "\n",
    "print('# Load PEFT config')\n",
    "config = PeftConfig.from_pretrained(lora_adapter_path)\n",
    "\n",
    "print('# Load base model')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_adapter_path)\n",
    "\n",
    "print('# Get PeftModel')\n",
    "peft_model = PeftModel.from_pretrained(base_model, lora_adapter_path, 'abcd')\n",
    "\n",
    "breakpoint()\n",
    "peft_model.set_adapter('abcd')  # set adapter as active\n",
    "\n",
    "print(\"# Merge adapter into model\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "breakpoint()\n",
    "# merged_model.save_pretrained(\"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")\n",
    "merged_model.save_pretrained(\"./tmp/Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")\n",
    "tokenizer.save_pretrained(\"./tmp/Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a08a8-6e8c-4a46-942e-f5436e26ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_model, tokenizer, peft_model, merged_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15163a06-c5b5-4db8-9a2e-4a46437c7b88",
   "metadata": {},
   "source": [
    "### DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fccec-c1fa-4eb3-8d16-9eaa046abbc4",
   "metadata": {},
   "source": [
    "We next further steer the above SFT LoRA model using DPO.\n",
    "\n",
    "In the example below, we use a preference dataset that is already in a conversational format needed by DPO so no preprocessing is neeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81782e73-0a60-4a51-9f63-817dfebf3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9eb6b6-1481-4efe-9f3d-0ce161569044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73de55f-834b-4719-8a22-6a98e69f0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cdf6b-4f0a-4f5a-a6d2-b184b20cdfcb",
   "metadata": {},
   "source": [
    "To use DPO, we import the corresponding DPO control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0a73a-a3ba-4853-9c22-40900b68bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ecae8-d5b7-434a-bdb5-ed7b75d75627",
   "metadata": {},
   "source": [
    "DPO steering is run the same way as SFT above. The DPO control is created and steering pipeline in invoked after providing the model name and control set to `dpo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a35fe7-53ab-4214-a131-c59cf36c59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "dpo = DPO(\n",
    "    train_dataset=dataset,\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"Qwen2.5-0.5B-DPO-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7da81-8f6b-4c7d-9d44-9df5ae831e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering pipeline\n",
    "dpo_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=model_name,\n",
    "    controls=[dpo],\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d4d7-7396-46a8-87e5-f9c1f3996ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14a4cb-214e-4db4-848f-e6a1edaf1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\")\n",
    "question = 'QUESION'+dataset[0]['chosen'][-2]['content'].rsplit('QUESTION',1)[-1]\n",
    "print(question)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "enc = tokenizer(text, return_tensors=\"pt\", padding=True, padding_side=\"left\").to(dpo_pipeline.model.device)\n",
    "steered_response = dpo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (DPO):\")\n",
    "print(steered_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de4b09-6b95-4a94-b3bf-ccfce57392f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "import gc\n",
    "del dpo_pipeline.model, dpo_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13defcb8-8bcc-4759-b7d1-b9349750b545",
   "metadata": {},
   "source": [
    "### APO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d593228-4bc2-4a31-afc3-fa4f02f7eb3d",
   "metadata": {},
   "source": [
    "Now, we demonstrate how to run APO with the same previously steered SFT LoRA model. APO is run in the same manner as DPO above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06ade-528f-4d60-a1a2-cf078eae3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen2.5-0.5B-SFT-LoRA-Steer-Merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ee6cb-a45d-4bbb-89c6-4f0cbed2573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n",
    "subset_size = 500\n",
    "dataset = dataset.select(list(range(subset_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f02bc1-c11e-4d4a-84a1-303c9656dc15",
   "metadata": {},
   "source": [
    "We use the APO control and set the SteeringPipeline with APO as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff50f1-75cb-4169-ac5c-a1f19bcf45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.apotrainer.control import APO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd44a3-b8db-4b0f-9d10-a2e748864729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "apo = APO(\n",
    "    train_dataset=dataset,\n",
    "    **{\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"output_dir\": \"Qwen2.5-0.5B-APO-Steer\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_strategy\": \"no\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f558a-da6d-491f-bbcb-441640b0d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering pipeline\n",
    "apo_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=model_name,\n",
    "    controls=[apo],\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce47ff-e0fc-46e2-b414-f6c261e47d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "apo_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6880c69-c636-4368-8c5f-35195081e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"test\")\n",
    "question = 'QUESION'+dataset[0]['chosen'][-2]['content'].rsplit('QUESTION',1)[-1]\n",
    "print(question)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "enc = tokenizer(text, return_tensors=\"pt\", padding=True, padding_side=\"left\").to(apo_pipeline.model.device)\n",
    "steered_response = apo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (APO):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31beee-daf8-43dd-8b38-eda5dbc37555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing memory resources\n",
    "del apo_pipeline.model, apo_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413397d-fa69-41a5-a36f-8331b6376c9a",
   "metadata": {},
   "source": [
    "### SPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e778e3-015f-42a6-9c05-7fa00184424b",
   "metadata": {},
   "source": [
    "To run SPPO, extra classes need to be imported, and multiple iterations of steering can be performed. The example below is based on the [SPPO paper](https://arxiv.org/abs/2405.00675) and the iteration code below is based on scripts from the [SPPO github repository](https://github.com/uclaml/SPPO/tree/main).\n",
    "\n",
    "The example shows 3 iterations of SPPO applied to a Mistral model using a Huggingface prompt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llm-blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5d5fb-419d-4e1c-92ae-23467df0602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.structural_control.wrappers.trl.sppotrainer.control import SPPO\n",
    "\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.sppotrainer.utils import (\n",
    "    set_seed,\n",
    "    apply_template,\n",
    "    ranking,\n",
    "    from_ranks,\n",
    "    prepare_score,\n",
    "    apply_chat_template,\n",
    "    process_dataset,\n",
    "    prepare_dataset_from_prompts\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642691b-a6b1-41a7-8996-6a8bc8f4f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SPPO(to_be_steered_model_path_or_name, data, sppo_temp_dir, start_iter_num=1, end_iter_num=1, maxlen = 2048, \n",
    "             num_prompts=5, additional_train_datasets=None):\n",
    "    checkpoints_path = \"\"\n",
    "    steerer = None\n",
    "\n",
    "    checkpoints_path=f\"{sppo_temp_dir}/checkpoints/SPPO-FINAL\"  #steered model stored at each iteration\n",
    "          \n",
    "\n",
    "    # Steer model\n",
    "    sppo = SPPO(\n",
    "        train_dataset=data,\n",
    "        eval_dataset=None,\n",
    "        **{\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"num_train_epochs\": 1,\n",
    "            \"learning_rate\": 5.0e-7,\n",
    "            \"output_dir\": checkpoints_path,\n",
    "            \"save_strategy\": \"no\",\n",
    "            \"beta\": 0.001,\n",
    "            \"optim\": \"rmsprop\",\n",
    "            \"loss_type\": \"sppo\",\n",
    "            \"max_prompt_length\": 128,\n",
    "            \"max_length\": 512\n",
    "    \n",
    "        },\n",
    "    )\n",
    "\n",
    "    # steerer\n",
    "    sppo_pipeline = SteeringPipeline(\n",
    "        model_name_or_path=to_be_steered_model_path_or_name,\n",
    "        controls=[sppo],\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "        hf_model_kwargs={\"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    "    )\n",
    "\n",
    "    sppo_pipeline.steer(num_prompts=num_prompts, start_iter_num=start_iter_num, end_iter_num=end_iter_num, \n",
    "                                  additional_train_datasets=additional_train_datasets, sppo_temp_dir=sppo_temp_dir, maxlen=maxlen)\n",
    "\n",
    "    return sppo_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed07cca-cdcb-48d7-9de9-88c26b58a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/uclaml/SPPO/blob/main/run_sppo_mistral.sh\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "start_iter_num = 1\n",
    "end_iter_num = 3\n",
    "num_prompts = 2  # number of responses to generate for each prompt (default is 5)\n",
    "\n",
    "\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"Qwen/Qwen2.5-0.5B-Instruct\" #\n",
    "\n",
    "\n",
    "prompt_datasets=[\"UCLA-AGI/data-mistral-7b-instruct-sppo-iter1\", \n",
    "                 \"UCLA-AGI/data-mistral-7b-instruct-sppo-iter2\", \n",
    "                 \"UCLA-AGI/data-mistral-7b-instruct-sppo-iter3\" ] #prompt datasets to be used\n",
    "\n",
    "\n",
    "m_name = BASE_MODEL.split(\"/\")[-1]\n",
    "sppo_temp_dir = m_name+\"_SPPO\"\n",
    "\n",
    "# We use just 10 records of each dataset for the demonstration\n",
    "subset_size = 10\n",
    "\n",
    "dataset = load_dataset(prompt_datasets[start_iter_num-1], split=\"train\")\n",
    "data = dataset.select(list(range(subset_size)))\n",
    "del dataset\n",
    "\n",
    "additional_train_datasets = []\n",
    "for dset in range(start_iter_num, end_iter_num):\n",
    "    dataset = load_dataset(prompt_datasets[dset], split=\"train\")\n",
    "    addl_data = dataset.select(list(range(subset_size)))\n",
    "    additional_train_datasets.append(addl_data)\n",
    "    del dataset\n",
    "\n",
    "\n",
    "\n",
    "if start_iter_num == 1:\n",
    "    to_be_steered_model_path_or_name = BASE_MODEL\n",
    "else:\n",
    "    to_be_steered_model_path_or_name = f\"{sppo_temp_dir}/checkpoints/SPPO-Iter{start_iter_num-1}\"\n",
    "\n",
    "\n",
    "\n",
    "sppo_pipeline = run_SPPO(to_be_steered_model_path_or_name, data=data, sppo_temp_dir=sppo_temp_dir, start_iter_num=start_iter_num, end_iter_num=end_iter_num, \n",
    "         additional_train_datasets=additional_train_datasets, num_prompts=num_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa49ebf-06b4-4e5b-9d6d-9a9bdf5f4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(f\"UCLA-AGI/data-mistral-7b-instruct-sppo-iter1\", split=\"train\")\n",
    "\n",
    "subset_size = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "prompt = apply_template(dataset[subset_size][\"prompt\"], tokenizer)\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\").to(sppo_pipeline.model.device)  \n",
    "\n",
    "steered_response = sppo_pipeline.generate_text(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"output (SPPO):\")\n",
    "print(steered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58430539-a6bb-4d51-ae0d-88d612fc4a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
