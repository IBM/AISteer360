{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb23221b",
   "metadata": {},
   "source": [
    "## Instruction Following\n",
    "\n",
    "This notebook provides a demonstration of building a benchmark to use steering to improve instruction following on the [Split-IFEval](https://huggingface.co/datasets/ibm-research/Split-IFEval) dataset. This notebook compares four steering pipelines: the unsteered behavior (baseline model), `PASTA`, `DeAL`, and `ThinkingIntervention`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24a6b5",
   "metadata": {},
   "source": [
    "### Building the use-case\n",
    "\n",
    "The instruction following use-case has already been constructed tutorial and is available at `aisteer360/evaluation/use_cases/instruction_following/use_case.py`. For details on how to construct use cases, please see the [tutorial](../../../docs/tutorials/add_new_use_case.md). The instruction following use case is initialized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ee4eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"key\": 1000,\n",
      "  \"prompt\": \"Write a summary of the wikipedia page \\\"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\\\".\\n\\nYour response should follow the instructions below:\\n- Write 300+ words\\n- Do not use any commas\\n- Highlight at least 3 sections that have titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*\",\n",
      "  \"instruction_id_list\": [\n",
      "    \"punctuation:no_comma\",\n",
      "    \"detectable_format:number_highlighted_sections\",\n",
      "    \"length_constraints:number_words\"\n",
      "  ],\n",
      "  \"kwargs\": [\n",
      "    {\n",
      "      \"num_bullets\": null,\n",
      "      \"num_highlights\": null,\n",
      "      \"relation\": null,\n",
      "      \"num_words\": null,\n",
      "      \"capital_relation\": null,\n",
      "      \"capital_frequency\": null,\n",
      "      \"num_sentences\": null,\n",
      "      \"end_phrase\": null,\n",
      "      \"keyword\": null,\n",
      "      \"frequency\": null,\n",
      "      \"prompt_to_repeat\": null,\n",
      "      \"first_word\": null,\n",
      "      \"num_paragraphs\": null,\n",
      "      \"nth_paragraph\": null,\n",
      "      \"let_relation\": null,\n",
      "      \"letter\": null,\n",
      "      \"let_frequency\": null,\n",
      "      \"section_spliter\": null,\n",
      "      \"num_sections\": null,\n",
      "      \"postscript_marker\": null,\n",
      "      \"forbidden_words\": null,\n",
      "      \"num_placeholders\": null,\n",
      "      \"language\": null,\n",
      "      \"keywords\": null\n",
      "    },\n",
      "    {\n",
      "      \"num_bullets\": null,\n",
      "      \"num_highlights\": 3,\n",
      "      \"relation\": null,\n",
      "      \"num_words\": null,\n",
      "      \"capital_relation\": null,\n",
      "      \"capital_frequency\": null,\n",
      "      \"num_sentences\": null,\n",
      "      \"end_phrase\": null,\n",
      "      \"keyword\": null,\n",
      "      \"frequency\": null,\n",
      "      \"prompt_to_repeat\": null,\n",
      "      \"first_word\": null,\n",
      "      \"num_paragraphs\": null,\n",
      "      \"nth_paragraph\": null,\n",
      "      \"let_relation\": null,\n",
      "      \"letter\": null,\n",
      "      \"let_frequency\": null,\n",
      "      \"section_spliter\": null,\n",
      "      \"num_sections\": null,\n",
      "      \"postscript_marker\": null,\n",
      "      \"forbidden_words\": null,\n",
      "      \"num_placeholders\": null,\n",
      "      \"language\": null,\n",
      "      \"keywords\": null\n",
      "    },\n",
      "    {\n",
      "      \"num_bullets\": null,\n",
      "      \"num_highlights\": null,\n",
      "      \"relation\": \"at least\",\n",
      "      \"num_words\": 300,\n",
      "      \"capital_relation\": null,\n",
      "      \"capital_frequency\": null,\n",
      "      \"num_sentences\": null,\n",
      "      \"end_phrase\": null,\n",
      "      \"keyword\": null,\n",
      "      \"frequency\": null,\n",
      "      \"prompt_to_repeat\": null,\n",
      "      \"first_word\": null,\n",
      "      \"num_paragraphs\": null,\n",
      "      \"nth_paragraph\": null,\n",
      "      \"let_relation\": null,\n",
      "      \"letter\": null,\n",
      "      \"let_frequency\": null,\n",
      "      \"section_spliter\": null,\n",
      "      \"num_sections\": null,\n",
      "      \"postscript_marker\": null,\n",
      "      \"forbidden_words\": null,\n",
      "      \"num_placeholders\": null,\n",
      "      \"language\": null,\n",
      "      \"keywords\": null\n",
      "    }\n",
      "  ],\n",
      "  \"separated_prompt\": \"Write a summary of the wikipedia page \\\"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\\\".\",\n",
      "  \"instructions\": [\n",
      "    \"- Write 300+ words\",\n",
      "    \"- Do not use any commas\",\n",
      "    \"- Highlight at least 3 sections that have titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*\"\n",
      "  ],\n",
      "  \"original_prompt\": \"Write a 300+ word summary of the wikipedia page \\\"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\\\". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from aisteer360.evaluation.use_cases.instruction_following import InstructionFollowing\n",
    "from aisteer360.evaluation.metrics.custom.instruction_following.strict_instruction import StrictInstruction\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# load the dataset and look at one example\n",
    "dataset = load_dataset(\"ibm-research/Split-IFEval\", split=\"train\")\n",
    "evaluation_data = dataset.to_list()\n",
    "print(json.dumps(evaluation_data[0], indent=2))\n",
    "\n",
    "# Define the instruction following use-case\n",
    "# Provide the evaluation data and the metric to be used.\n",
    "instruction_following = InstructionFollowing(\n",
    "    evaluation_data=evaluation_data,\n",
    "    evaluation_metrics=[StrictInstruction()],\n",
    "    num_samples=50  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c709b4",
   "metadata": {},
   "source": [
    "The goal of this use case is to evaluate the ability of LLMs to follow instructions provided in the prompt. From the example above, we see that the task `\"Write a 300+ word summary of the wikipedia page...\"` contains various instructions for the model (requiring a response of more than 300 words, with no commas, and at least 3 highlighted sections).\n",
    "\n",
    "**Note**: The original IFEval dataset specifies instructions *within* the prompt/task itself, making it difficult to evaluate certain steering methods (e.g., PASTA which requires steering attentions on *only* the instruction tokens). As a result, we have modified the dataset to extract the instructions for each prompt/task (see [https://huggingface.co/datasets/ibm-research/Split-IFEval](https://huggingface.co/datasets/ibm-research/Split-IFEval) for the modified dataset).\n",
    "\n",
    "We use the IFEval metric, `StrictInstruction`, which returns two values:\n",
    "1. prompt-level accuracy: measures the percentage of instances where all instructions were followed, and\n",
    "2. instruction-level accuracy: measures the overall instruction following accuracy as a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad18a4",
   "metadata": {},
   "source": [
    "### Defining the controls\n",
    "\n",
    "This demonstration aims to compare the baseline model prediction against three steering methods: `PASTA`, `DeAL`, and `ThinkingIntervention`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a70340-afcd-41ed-9e72-fd7aa03dd16a",
   "metadata": {},
   "source": [
    "#### Defining the PASTA control\n",
    "\n",
    "For PASTA, we choose to apply the attention steering bias to layers 8 and 9, and set the amount of bias to 0.01. We also assign `scale_position=exclude` to indicate that attention should be scaled away from the target substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb41900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.state_control.pasta.control import PASTA\n",
    "pasta = PASTA(\n",
    "        head_config=[8,9],\n",
    "        alpha=0.01,\n",
    "        scale_position=\"exclude\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03417be-55c5-45b9-b0ce-532fbb7fcf1d",
   "metadata": {},
   "source": [
    "#### Defining the DeAL control\n",
    "\n",
    "With DeAL, we define the number of lookahead tokens and beams. At each step we choose the top-4 beams (based on the reward scores), and repeat this for at most `max_iterations=10`.\n",
    "\n",
    "We use the `StrictInstruction` metric as the reward function which rewards beams that are most aligned with the instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39864bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.evaluation.metrics.custom.instruction_following.strict_instruction import StrictInstruction\n",
    "\n",
    "\n",
    "def strict_reward(prompt: str, responses: list[str], params: dict) -> list[float]:\n",
    "    \"\"\"DeAL reward function based on IFEval's strict instruction metric.\n",
    "    Used for beam selection in the instruction following benchmark use-case.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input + generation until current step\n",
    "        responses (list[str]): Beam responses for the given prompt\n",
    "        params (dict): DeAL parameters and input dataset fields\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward scores for each beam\n",
    "    \"\"\"\n",
    "    metric = StrictInstruction()\n",
    "    assert all(\n",
    "        key in params for key in [\"instructions\", \"instruction_id_list\", \"kwargs\"]\n",
    "    ), f\"Missing parameters for evaluation\"\n",
    "    accuracies = [\n",
    "        metric.compute(\n",
    "            predictions=[\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": response,\n",
    "                    \"instruction_id_list\": params[\"instruction_id_list\"],\n",
    "                    \"instructions\": params[\"instructions\"],\n",
    "                    \"kwargs\": params[\"kwargs\"],\n",
    "                }\n",
    "            ]\n",
    "        )[\"strict_instruction_accuracy\"]\n",
    "        for response in responses\n",
    "    ]\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700c7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.output_control.deal.control import DeAL\n",
    "\n",
    "deal = DeAL(\n",
    "        lookahead=50, \n",
    "        init_beams=8, \n",
    "        topk=2, \n",
    "        max_iterations=20, \n",
    "        reward_func=strict_reward\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec605333-a849-4c52-bc7c-464235d16537",
   "metadata": {},
   "source": [
    "#### Defining the Thinking Intervention control\n",
    "\n",
    "The `ThinkingIntervention` control adds a task-specific string (an intervention) to the model response just before generation. For this use case, we have defined the following intervention function to remind the model to follow the specified instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc40af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_following_intervention(prompt: str, params: dict) -> str:\n",
    "        intervention = (\n",
    "            \"I will first think using the <think> and </think> tags and then provide the final answer after that.\\n\"\n",
    "            \"<think> I should ensure that the answer follows these instructions. \"\n",
    "        )\n",
    "        modified_instr = [instr.replace(\"-\", \"\") for instr in params[\"instructions\"]]\n",
    "        intervention += \" and\".join(modified_instr)\n",
    "        return prompt + intervention + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50a65d",
   "metadata": {},
   "source": [
    "Now we can define the thinking intervention control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51fb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisteer360.algorithms.output_control.thinking_intervention.control import ThinkingIntervention\n",
    "\n",
    "thinking_intervention = ThinkingIntervention(\n",
    "    intervention=instruction_following_intervention\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54b33e",
   "metadata": {},
   "source": [
    "### Instantiating (and running) the benchmark\n",
    "\n",
    "The benchmark is instantiated with the `instruction_following` use case and the three controls (above) plus a baseline (empty pipeline). The base model is `Qwen/Qwen2.5-1.5B-Instruct`.\n",
    "\n",
    "**Note**: `PASTA`, `DeAL` and `ThinkingIntervention` each require specific arguments in order to execute. For instance, `PASTA` needs to know which `substrings` to emphasize (the `instructions` in the dataset for this particular use-case), `DeAL` requires all the keys from the input dataset needed to compute the reward, and the `ThinkingIntervention` helper requires the instructions from the `params` argument. These arguments, termed `runtime_kwargs`, are populated from the IFEval dataset via the `runtime_overrides` parameter (which tells the benchmark how to populate the `runtime_kwargs` from the columns of the evaluation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efc633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline: baseline...\n",
      "done.\n",
      "Running pipeline: pasta...\n",
      "done.\n",
      "Running pipeline: deal...\n",
      "done.\n",
      "Running pipeline: thinking_intervention...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from aisteer360.evaluation.benchmark import Benchmark\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "benchmark = Benchmark(\n",
    "        use_case=instruction_following,\n",
    "        base_model_name_or_path=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        steering_pipelines={\n",
    "            \"baseline\": [], # no steering\n",
    "            \"pasta\": [pasta],\n",
    "            \"deal\": [deal],\n",
    "            \"thinking_intervention\": [thinking_intervention]\n",
    "        },\n",
    "        runtime_overrides={\n",
    "            \"PASTA\": {\"substrings\": \"instructions\"},\n",
    "            \"DeAL\": {\n",
    "                \"reward_params\": {\n",
    "                    \"instruction_id_list\": \"instruction_id_list\",\n",
    "                    \"instructions\": \"instructions\",\n",
    "                    \"kwargs\": \"kwargs\",\n",
    "                }\n",
    "            },\n",
    "            \"ThinkingIntervention\": {\"params\": {\"instructions\": \"instructions\"}},\n",
    "        },\n",
    "        gen_kwargs={\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"do_sample\": False,\n",
    "            \"output_attentions\": True,  # mandatory for PASTA\n",
    "        },\n",
    "        hf_model_kwargs={\"attn_implementation\": \"eager\"}, # mandatory for PASTA\n",
    "    )\n",
    "\n",
    "profiles = benchmark.run()\n",
    "benchmark.export(profiles=profiles, save_dir=\"./results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c90598",
   "metadata": {},
   "source": [
    "### Inspecting the profiles\n",
    "\n",
    "Each steering pipeline in the benchmark yields an evaluation profile. Each evaluation profile contains metric values as computed by the metrics passed in to the use case, in this case `StrictInstruction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8db3ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"StrictInstruction\": {\n",
      "    \"strict_prompt_accuracy\": 0.5,\n",
      "    \"strict_instruction_accuracy\": 0.5131578947368421\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(profiles['baseline']['evaluations'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1356279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"StrictInstruction\": {\n",
      "    \"strict_prompt_accuracy\": 0.2,\n",
      "    \"strict_instruction_accuracy\": 0.27631578947368424\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(profiles['pasta']['evaluations'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ae4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"StrictInstruction\": {\n",
      "    \"strict_prompt_accuracy\": 0.2,\n",
      "    \"strict_instruction_accuracy\": 0.3157894736842105\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(profiles['deal']['evaluations'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b670f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"StrictInstruction\": {\n",
      "    \"strict_prompt_accuracy\": 0.26,\n",
      "    \"strict_instruction_accuracy\": 0.4473684210526316\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(profiles['thinking_intervention']['evaluations'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e07f1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
